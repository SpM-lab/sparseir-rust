//! Kernel implementations for SparseIR
//!
//! This module provides kernel implementations for analytical continuation in quantum many-body physics.
//! The kernels are used in Fredholm integral equations of the first kind.
//!
//! u(x) = integral of K(x, y) v(y) dy
//!
//! where x ∈ [xmin, xmax] and y ∈ [ymin, ymax].
//!
//! In general, the kernel is applied to a scaled spectral function rho'(y) as:
//!
//! integral of K(x, y) rho'(y) dy,
//!
//! where ρ'(y) = w(y) ρ(y). The weight function w(y) transforms the original spectral
//! function ρ(y) into the scaled version ρ'(y) used in the integral equation.

use twofloat::TwoFloat;
use crate::traits::{StatisticsType, Statistics, Fermionic, Bosonic};
use crate::gauss::Rule;
use crate::numeric::CustomNumeric;
use ndarray::Array2;
use std::fmt::Debug;
use num_traits::ToPrimitive;
use std::ops::{Index, IndexMut, Sub};
use rayon::prelude::*;


enum Parity {
    Even = 1,
    Odd = -1
}

/// Trait for SVE (Singular Value Expansion) hints
/// 
/// Provides discretization hints for singular value expansion of a given kernel.
/// This includes segment information and numerical parameters for efficient computation.
pub trait SVEHints<T>: Debug + Send + Sync
where
    T: Copy + Debug + Send + Sync,
{
    /// Get the x-axis segments for discretization
    fn segments_x(&self) -> Vec<T>;
    
    /// Get the y-axis segments for discretization
    fn segments_y(&self) -> Vec<T>;
    
    /// Get the number of singular values hint
    fn nsvals(&self) -> usize;
    
    /// Get the number of Gauss points for quadrature
    fn ngauss(&self) -> usize;
}

/// Trait for kernel type properties (static characteristics)
pub trait KernelProperties {
    /// Associated type for SVE hints
    type SVEHintsType<T>: SVEHints<T> + Clone
    where
        T: Copy + Debug + Send + Sync + CustomNumeric + 'static;
    /// Power with which the y coordinate scales.
    /// 
    /// For most kernels, this is 0 (no scaling).
    /// For RegularizedBoseKernel, this is 1 (linear scaling).
    fn ypower(&self) -> i32;
    
    /// Get the upper bound of the x domain
    fn xmax(&self) -> f64;
    
    /// Get the upper bound of the y domain
    fn ymin(&self) -> f64;
    
    /// Weight function for given statistics.
    /// 
    /// The kernel is applied to a scaled spectral function ρ'(y) as:
    ///     ∫ K(x, y) ρ'(y) dy,
    /// where ρ'(y) = w(y) ρ(y).
    /// 
    /// This function returns w(beta, omega) that transforms the original spectral
    /// function ρ(y) into the scaled version ρ'(y) used in the integral equation.
    /// 
    /// @param beta Inverse temperature
    /// @param omega Frequency
    /// @return The weight value w(beta, omega)
    fn weight<S: StatisticsType + 'static>(&self, beta: f64, omega: f64) -> f64;
    
    /// Inverse weight function to avoid division by zero.
    /// 
    /// This is a safer API that returns the inverse weight.
    /// 
    /// @param beta Inverse temperature  
    /// @param omega Frequency
    /// @return The inverse weight value
    fn inv_weight<S: StatisticsType + 'static>(&self, beta: f64, omega: f64) -> f64;
    
    /// Create SVE hints for this kernel type.
    /// 
    /// Provides discretization hints for singular value expansion computation.
    /// The hints include segment information and numerical parameters optimized
    /// for the specific kernel type.
    /// 
    /// @param epsilon Target accuracy for the SVE computation
    /// @return SVE hints specific to this kernel type
    fn sve_hints<T>(&self, epsilon: f64) -> Self::SVEHintsType<T>
    where
        T: Copy + Debug + Send + Sync + CustomNumeric + 'static;
}

/// trait for centrosymmetric kernerl
pub trait CentrosymmKernel: Send + Sync {
    /// Compute the kernel value K(x, y) with high precision
    fn compute(&self, x: TwoFloat, y: TwoFloat) -> TwoFloat;

    fn compute_f64(&self, x: f64, y: f64) -> f64;

    /// Compute the reduced kernel value
    // K_red(x, y) = K(x, y) + sign * K(x, -y)
    fn compute_reduced(&self, x: TwoFloat, y: TwoFloat, sign: Parity) -> TwoFloat;

    fn compute_reduced_f64(&self, x: f64, y: f64, sign: Parity) -> f64;
    
    /// Get the cutoff parameter Λ (lambda)
    fn lambda(&self) -> f64;
    
    /// Convergence radius of the Matsubara basis asymptotic model.
    /// 
    /// For improved relative numerical accuracy, the IR basis functions on the
    /// Matsubara axis can be evaluated from an asymptotic expression for
    /// abs(n) > conv_radius. If conv_radius is infinity, then the asymptotics
    /// are unused (the default).
    fn conv_radius(&self) -> f64;
}

/// Logistic kernel for fermionic analytical continuation
/// 
/// This kernel implements K(x, y) = exp(-Λy(x + 1)/2)/(1 + exp(-Λy))
/// where x ∈ [-1, 1] and y ∈ [-1, 1]
#[derive(Debug, Clone, Copy)]
pub struct LogisticKernel {
    lambda: f64,
}

impl LogisticKernel {
    /// Create a new logistic kernel with the given cutoff parameter
    pub fn new(lambda: f64) -> Self {
        Self { lambda }
    }
    
    /// Get the cutoff parameter
    pub fn lambda(&self) -> f64 {
        self.lambda
    }
}

impl KernelProperties for LogisticKernel {
    type SVEHintsType<T> = LogisticSVEHints<T>
    where
        T: Copy + Debug + Send + Sync + CustomNumeric + 'static;
    fn ypower(&self) -> i32 {
        0 // No y-power scaling for LogisticKernel
    }
    
    fn xmax(&self) -> f64 { 1.0 }
    fn ymin(&self) -> f64 { 1.0 }
    
    fn weight<S: StatisticsType + 'static>(&self, beta: f64, omega: f64) -> f64 {
        match S::STATISTICS {
            Statistics::Fermionic => {
                // For fermionic statistics: w(beta, omega) = 1.0
                // The kernel K(x, y) is already in the correct form for fermions
                1.0
            }
            Statistics::Bosonic => {
                // For bosonic statistics: w(beta, omega) = 1.0 / tanh(0.5 * beta * omega)
                // This transforms the fermionic kernel to work with bosonic correlation functions
                // The tanh factor accounts for the different statistics
                1.0 / (0.5 * beta * omega).tanh()
            }
        }
    }
    
    fn inv_weight<S: StatisticsType + 'static>(&self, beta: f64, omega: f64) -> f64 {
        match S::STATISTICS {
            Statistics::Fermionic => {
                // For fermionic statistics: 1/w = 1.0 (safe, no division by zero)
                1.0
            }
            Statistics::Bosonic => {
                // For bosonic statistics: 1/w = tanh(0.5 * beta * omega) (safe, handles omega=0 case)
                // This avoids division by zero when tanh(0.5 * beta * omega) approaches zero
                (0.5 * beta * omega).tanh()
            }
        }
    }
    
    fn sve_hints<T>(&self, epsilon: f64) -> Self::SVEHintsType<T>
    where
        T: Copy + Debug + Send + Sync + CustomNumeric + 'static,
    {
        LogisticSVEHints::new(self.clone(), epsilon)
    }
}

fn compute_logistic_kernel<T: CustomNumeric>(lambda: f64, x: T, y: T) {
    let x_plus: T = 1.0 + x;
    let x_minus: T = 1.0 - x;

    let u_plus: T = 0.5 * x_plus;
    let u_minus: T = 0.5 * x_minus;
    let v: T = lambda * y;

    let mabs_v: T = -v.abs();
    let numerator: T = if (v >= 0) {
        numerator = (u_plus * mabs_v).exp();
    } else {
        numerator = (u_minus * mabs_v).exp();
    };
    let denominator: T = 1.0 + (mabs_v).exp();
    return numerator / denominator;
}

fn compute_logistic_kernel_reduced_odd<T: CustomNumeric>(lambda: f64, x: T, y: T) {
    // For x * y around 0, antisymmetrization introduces cancellation, which
    // reduces the relative precision. To combat this, we replace the
    // values with the explicit form
    let v_half: T = lambda * 0.5 * y;
    let xy_small: bool = x * v_half < 1;
    let cosh_finite: bool = v_half < 85;
    if (xy_small && cosh_finite) {
        return -sinh(v_half * x) / cosh(v_half);
    } else {
        let k_plus = compute_logistic_kernel(lambda, x, y);
        let k_minus = compute_logistic_kernel(lambda, x, -y);
        return k_plus - k_minus;
    }
}

impl CentrosymmKernel for LogisticKernel {
    fn compute(&self, x: TwoFloat, y: TwoFloat) -> TwoFloat {
        return compute_logistic_kernel(lambda, x, y);
    }

    fn compute_f64(&self, x: f64, y: f64) -> f64 {
        return compute_logistic_kernel(lambda, x, y);
    }

    fn compute_reduced(&self, x: TwoFloat, y: TwoFloat, sign: Parity) -> TwoFloat {
        match sign {
            Parity::Even => self.compute(x, y) + self.compute(x, -y),
            Parity::Odd => compute_logistic_kernel_reduced_odd(self.lambda, x, y),
        }
    }

    fn compute_reduced_f64(&self, x: f64, y: f64, sign: Parity) -> f64 {
        match sign {
            Parity::Even => self.compute_f64(x, y) + self.compute_f64(x, -y),
            Parity::Odd => compute_logistic_kernel_reduced_odd(self.lambda, x, y),
        }
    }
    
    fn lambda(&self) -> f64 { self.lambda }
    
    fn conv_radius(&self) -> f64 {
        40.0 * self.lambda // For LogisticKernel, conv_radius = 40 * Λ
    }
}

/// SVE hints for LogisticKernel
#[derive(Debug, Clone)]
pub struct LogisticSVEHints<T> {
    kernel: LogisticKernel,
    epsilon: f64,
    _phantom: std::marker::PhantomData<T>,
}

impl<T> LogisticSVEHints<T>
where
    T: Copy + Debug + Send + Sync,
{
    pub fn new(kernel: LogisticKernel, epsilon: f64) -> Self {
        Self {
            kernel,
            epsilon,
            _phantom: std::marker::PhantomData,
        }
    }
}

impl<T> SVEHints<T> for LogisticSVEHints<T>
where
    T: Copy + Debug + Send + Sync + CustomNumeric,
{
    fn segments_x(&self) -> Vec<T> {
        // Simplified implementation - in practice, this would use the full algorithm
        // from the C++ implementation with cosh calculations
        // TOAI: Implement exactly the same logic in the C++ code, return only >= 0
        let nzeros = std::cmp::max(
            (15.0 * self.kernel.lambda().log10()).round() as usize, 1
        );
        
        let mut segments = Vec::with_capacity(nzeros);
        
        // Create symmetric segments around 0
        for i in 0..=nzeros {
            let pos = <T as CustomNumeric>::from_f64(0.1 * i as f64);
            if i == 0 {
                segments.push(<T as CustomNumeric>::from_f64(0.0));
            } else {
                segments.push(pos);
            }
        }
        
        segments.sort_by(|a, b| a.to_f64().partial_cmp(&b.to_f64()).unwrap_or(std::cmp::Ordering::Equal));
        segments
    }
    
    fn segments_y(&self) -> Vec<T> {
        // C++ equivalent implementation from SVEHintsLogistic::segments_y
        // TOAI: Implement exactly the same logic in the C++ code, return only >= 0
        let nzeros = std::cmp::max(
            (20.0 * self.kernel.lambda().log10()).round() as usize, 2
        );
        
        // Initial differences (from C++ implementation)
        let mut diffs = vec![
            0.01523, 0.03314, 0.04848, 0.05987, 0.06703,
            0.07028, 0.07030, 0.06791, 0.06391, 0.05896,
            0.05358, 0.04814, 0.04288, 0.03795, 0.03342,
            0.02932, 0.02565, 0.02239, 0.01951, 0.01699
        ];
        
        // Truncate diffs if necessary
        if nzeros < diffs.len() {
            diffs.truncate(nzeros);
        }
        
        // Calculate trailing differences
        for i in 20..nzeros {
            let x = 0.141 * i as f64;
            diffs.push(0.25 * (-x).exp());
        }
        
        // Calculate cumulative sum of diffs
        let mut zeros = Vec::with_capacity(nzeros);
        zeros.push(diffs[0]);
        for i in 1..nzeros {
            zeros.push(zeros[i - 1] + diffs[i]);
        }
        
        // Normalize zeros
        let last_zero = zeros[nzeros - 1];
        for i in 0..nzeros {
            zeros[i] /= last_zero;
        }
        zeros.pop(); // Remove last element
        
        // Updated nzeros
        let nzeros = zeros.len();
        
        // Adjust zeros
        for i in 0..nzeros {
            zeros[i] -= 1.0;
        }
        
        // Create the final segments vector (2 * nzeros + 3 elements)
        let mut segments = vec![<T as CustomNumeric>::from_f64(0.0); 2 * nzeros + 3];
        
        for i in 0..nzeros {
            segments[1 + i] = <T as CustomNumeric>::from_f64(zeros[i]);
            segments[1 + nzeros + 1 + i] = <T as CustomNumeric>::from_f64(-zeros[nzeros - i - 1]);
        }
        
        segments[0] = <T as CustomNumeric>::from_f64(-1.0);
        segments[1 + nzeros] = <T as CustomNumeric>::from_f64(0.0);
        segments[2 * nzeros + 2] = <T as CustomNumeric>::from_f64(1.0);

        symm_segments(&segments)
    }
    
    fn nsvals(&self) -> usize {
        let log10_lambda = self.kernel.lambda().log10().max(1.0);
        ((25.0 + log10_lambda) * log10_lambda).round() as usize
    }
    
    fn ngauss(&self) -> usize {
        if self.epsilon >= 1e-8 { 10 } else { 16 }
    }
}


/// Function to validate symmetry and extract the positive half of segments
/// This is equivalent to C++ symm_segments function
fn symm_segments<T: CustomNumeric + Copy + Debug + Send + Sync>(segments: &[T]) -> Vec<T> {
    let n = segments.len();
    
    // Check if the vector is symmetric
    for i in 0..n / 2 {
        let left = segments[i].to_f64();
        let right = segments[n - i - 1].to_f64();
        if (left + right).abs() > f64::EPSILON {
            panic!("segments must be symmetric: segments[{}] = {}, segments[{}] = {}", 
                   i, left, n - i - 1, right);
        }
    }
    
    // Extract the second half of the vector starting from the middle
    let mid = n / 2;
    let mut xpos: Vec<T> = segments[mid..].to_vec();
    
    // Ensure the first element of xpos is zero; if not, prepend zero
    if xpos.is_empty() || xpos[0].to_f64().abs() > f64::EPSILON {
        xpos.insert(0, <T as CustomNumeric>::from_f64(0.0));
    }
    
    xpos
}
